{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65298a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 975 per-run summary rows to summaries.csv\n",
      "Wrote aggregated top-line summary to aggregated_topline.csv\n",
      "                  problem           model        prompt_type  pass_rate  \\\n",
      "0   DirectedSocialNetwork   Claude35Haiku       directprompt        0.0   \n",
      "1   DirectedSocialNetwork   Claude35Haiku  iterativefeedback        0.0   \n",
      "2   DirectedSocialNetwork   Claude35Haiku   programaugmented        0.0   \n",
      "3   DirectedSocialNetwork  Claude35Sonnet       directprompt      100.0   \n",
      "4   DirectedSocialNetwork  Claude35Sonnet  iterativefeedback      100.0   \n",
      "5   DirectedSocialNetwork  Claude35Sonnet   programaugmented       60.0   \n",
      "6   DirectedSocialNetwork      DeepSeekR1       directprompt       20.0   \n",
      "7   DirectedSocialNetwork      DeepSeekR1  iterativefeedback       40.0   \n",
      "8   DirectedSocialNetwork      DeepSeekR1   programaugmented        0.0   \n",
      "9   DirectedSocialNetwork      DeepSeekV3       directprompt        0.0   \n",
      "10  DirectedSocialNetwork      DeepSeekV3  iterativefeedback        0.0   \n",
      "11  DirectedSocialNetwork      DeepSeekV3   programaugmented        0.0   \n",
      "12  DirectedSocialNetwork           GPT4o       directprompt        0.0   \n",
      "13  DirectedSocialNetwork           GPT4o  iterativefeedback        0.0   \n",
      "14  DirectedSocialNetwork           GPT4o   programaugmented        0.0   \n",
      "15  DirectedSocialNetwork       GPT4omini       directprompt        0.0   \n",
      "16  DirectedSocialNetwork       GPT4omini  iterativefeedback        0.0   \n",
      "17  DirectedSocialNetwork       GPT4omini   programaugmented        0.0   \n",
      "18  DirectedSocialNetwork   Gemini20Flash       directprompt        0.0   \n",
      "19  DirectedSocialNetwork   Gemini20Flash  iterativefeedback       40.0   \n",
      "\n",
      "    avg_error_count  avg_structural_errors  avg_logical_errors  \\\n",
      "0               1.0                    0.0                 0.0   \n",
      "1               1.0                    0.0                 0.0   \n",
      "2               1.0                    1.0                 0.0   \n",
      "3               0.0                    0.0                 0.0   \n",
      "4               0.0                    0.0                 0.0   \n",
      "5               0.4                    0.0                 0.0   \n",
      "6               3.6                    0.8                 2.0   \n",
      "7               1.8                    0.2                 1.0   \n",
      "8               7.0                    1.0                 5.0   \n",
      "9               1.8                    0.4                 0.4   \n",
      "10              1.4                    0.4                 0.0   \n",
      "11              5.8                    0.6                 4.2   \n",
      "12              6.6                    0.6                 5.0   \n",
      "13              1.6                    0.2                 0.4   \n",
      "14              4.8                    0.4                 3.4   \n",
      "15              7.8                    0.4                 6.4   \n",
      "16              5.8                    0.6                 4.2   \n",
      "17              1.8                    0.2                 0.6   \n",
      "18              2.2                    0.0                 1.4   \n",
      "19              0.6                    0.0                 0.0   \n",
      "\n",
      "    avg_attribute_errors  avg_structural_passed  avg_logical_passed  \\\n",
      "0                    1.0                    3.0                 2.0   \n",
      "1                    1.0                    3.0                 2.0   \n",
      "2                    0.0                    2.0                 2.0   \n",
      "3                    0.0                    3.0                 2.0   \n",
      "4                    0.0                    3.0                 2.0   \n",
      "5                    0.4                    3.0                 2.0   \n",
      "6                    0.8                    2.2                 1.4   \n",
      "7                    0.6                    2.8                 1.6   \n",
      "8                    1.0                    2.0                 0.8   \n",
      "9                    1.0                    2.6                 1.6   \n",
      "10                   1.0                    2.6                 2.0   \n",
      "11                   1.0                    2.4                 0.6   \n",
      "12                   1.0                    2.4                 0.6   \n",
      "13                   1.0                    2.8                 1.6   \n",
      "14                   1.0                    2.6                 0.8   \n",
      "15                   1.0                    2.6                 0.8   \n",
      "16                   1.0                    2.4                 0.8   \n",
      "17                   1.0                    2.8                 1.8   \n",
      "18                   0.8                    3.0                 1.2   \n",
      "19                   0.6                    3.0                 2.0   \n",
      "\n",
      "    avg_attribute_passed  run_count  \n",
      "0                    3.0          5  \n",
      "1                    3.0          5  \n",
      "2                    4.0          5  \n",
      "3                    4.0          5  \n",
      "4                    4.0          5  \n",
      "5                    3.6          5  \n",
      "6                    3.2          5  \n",
      "7                    3.4          5  \n",
      "8                    3.0          5  \n",
      "9                    3.0          5  \n",
      "10                   3.0          5  \n",
      "11                   3.0          5  \n",
      "12                   3.0          5  \n",
      "13                   3.0          5  \n",
      "14                   3.0          5  \n",
      "15                   3.0          5  \n",
      "16                   3.0          5  \n",
      "17                   3.0          5  \n",
      "18                   3.2          5  \n",
      "19                   3.4          5  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "###############################################################################\n",
    "# 1. FILENAME PATTERN & PROBLEM MAP\n",
    "###############################################################################\n",
    "\n",
    "FILENAME_REGEX = re.compile(r'^(.*?)_(.*?)_(.*?)_run(\\d+)_output\\.json$')\n",
    "\n",
    "PROBLEM_NAME_MAP = {\n",
    "    \"cities\": \"OptimalNetworkDesign\",\n",
    "    \"directedSocialNetwork\": \"DirectedSocialNetwork\",\n",
    "    \"geneAssociation\": \"GeneDiseaseAssociation\",\n",
    "    \"quantum\": \"QuantumCircuit\",\n",
    "    \"timeDependentDeliveryNetwork\": \"TimeDependentDelivery\"\n",
    "}\n",
    "\n",
    "INPUT_GLOB = \"./**/*.json\"\n",
    "PER_RUN_CSV = \"summaries.csv\"\n",
    "AGGREGATED_CSV = \"aggregated_topline.csv\"\n",
    "\n",
    "###############################################################################\n",
    "# 2. SPLIT-ON-PERIOD HELPER\n",
    "###############################################################################\n",
    "\n",
    "def split_by_period_count(msg: str) -> int:\n",
    "    \"\"\"\n",
    "    Splits a message string on '.' and counts the non-empty segments.\n",
    "    This is how we detect multiple distinct error statements in a single constraint message.\n",
    "    \"\"\"\n",
    "    parts = [p.strip() for p in msg.split('.') if p.strip()]\n",
    "    return len(parts)\n",
    "\n",
    "###############################################################################\n",
    "# 3. GROUP MAPPING\n",
    "###############################################################################\n",
    "# Includes constraints from all problems, including \"cities\" (OptimalNetworkDesign).\n",
    "# We map each known constraint key to one of the three groups.\n",
    "\n",
    "CONSTRAINT_GROUPS = {\n",
    "    # Structural group\n",
    "    \"connectivity\": \"Structural\",\n",
    "    \"road_capacity\": \"Structural\",\n",
    "    \"redundancy\": \"Structural\",\n",
    "    \"max_edges_constraint\": \"Structural\",\n",
    "    \"valid_no_self_loops\": \"Structural\",\n",
    "    \"valid_edge_structure\": \"Structural\",\n",
    "    \"valid_acyclic\": \"Structural\",\n",
    "    \"bipartite_constraint\": \"Structural\",\n",
    "    \"duplicate_associations\": \"Structural\",\n",
    "    \"valid_dag\": \"Structural\",\n",
    "    \"layered_operations\": \"Structural\",\n",
    "    \"degree_constraints\" : \"Structural\",\n",
    "\n",
    "    # Logical group\n",
    "    \"cost_optimization\": \"Logical\",\n",
    "    \"population_accessibility\": \"Logical\",\n",
    "    \"strategic_road_placement\": \"Logical\",\n",
    "    \"valid_celebrity_outgoing\": \"Logical\",\n",
    "    \"valid_regular_to_expert\": \"Logical\",\n",
    "    \"time_window_compliance\": \"Logical\",\n",
    "    \"storage_capacity_compliance\": \"Logical\",\n",
    "    \"vehicle_capacity_compliance\": \"Logical\",\n",
    "    \"valid_gate_precedences\": \"Logical\",\n",
    "    \"valid_cnot_adjacency\": \"Logical\",\n",
    "    \"valid_swap_constraints\": \"Logical\",\n",
    "    \"valid_associations\": \"Logical\",\n",
    "    \n",
    "    # Attribute group\n",
    "    \"output_structure\": \"Attribute\",\n",
    "    \"valid_user_attributes\": \"Attribute\",\n",
    "    \"valid_total_users\": \"Attribute\",\n",
    "    \"valid_categories\": \"Attribute\",\n",
    "    \"valid_trust_scores\": \"Attribute\",\n",
    "    \"defined_counts\": \"Attribute\",\n",
    "    \"valid_gate_types\": \"Attribute\",\n",
    "    \"valid_qubits\": \"Attribute\",\n",
    "}\n",
    "\n",
    "###############################################################################\n",
    "# 4. COUNTING ERRORS & PASSED CONSTRAINTS\n",
    "###############################################################################\n",
    "\n",
    "def count_constraints_errors(fullOutput: dict) -> int:\n",
    "    \"\"\"\n",
    "    Count the total number of errors by scanning each constraint (top-level & nested).\n",
    "    If the constraint is \"passed\": false, we split its 'message' on '.' to count sub-errors.\n",
    "    If no message, count it as 1 error.\n",
    "\n",
    "    We do NOT look at fullOutput[\"errors\"].\n",
    "    \"\"\"\n",
    "    total_errors = 0\n",
    "\n",
    "    # Check top-level constraints\n",
    "    for key, val in fullOutput.items():\n",
    "        if key in [\"errors\", \"overall_passed\"]:\n",
    "            continue\n",
    "        if isinstance(val, dict) and \"passed\" in val:\n",
    "            if not val[\"passed\"]:\n",
    "                msg = val.get(\"message\", \"\")\n",
    "                count = split_by_period_count(msg) if msg else 1\n",
    "                total_errors += count\n",
    "        elif isinstance(val, bool):\n",
    "            if val is False:\n",
    "                total_errors += 1\n",
    "\n",
    "    # Check nested constraints under \"constraints\"\n",
    "    nested = fullOutput.get(\"constraints\", {})\n",
    "    if isinstance(nested, dict):\n",
    "        for key, val in nested.items():\n",
    "            if isinstance(val, dict) and \"passed\" in val:\n",
    "                if not val[\"passed\"]:\n",
    "                    msg = val.get(\"message\", \"\")\n",
    "                    count = split_by_period_count(msg) if msg else 1\n",
    "                    total_errors += count\n",
    "            elif isinstance(val, bool):\n",
    "                if val is False:\n",
    "                    total_errors += 1\n",
    "\n",
    "    return total_errors\n",
    "\n",
    "def count_grouped_errors(fullOutput: dict) -> dict:\n",
    "    \"\"\"\n",
    "    For each constraint in fullOutput and fullOutput[\"constraints\"] that fails,\n",
    "    find which group it belongs to (if any), and count sub-errors by splitting on '.'.\n",
    "    If no message, count as 1. Return a dict: { \"Structural\": X, \"Logical\": Y, \"Attribute\": Z }\n",
    "    \"\"\"\n",
    "    groups = {\"Structural\": 0, \"Logical\": 0, \"Attribute\": 0}\n",
    "\n",
    "    # Top-level\n",
    "    for key, val in fullOutput.items():\n",
    "        if key in [\"errors\", \"overall_passed\"]:\n",
    "            continue\n",
    "        group = CONSTRAINT_GROUPS.get(key)\n",
    "        if group:\n",
    "            if isinstance(val, dict) and \"passed\" in val:\n",
    "                if not val[\"passed\"]:\n",
    "                    msg = val.get(\"message\", \"\")\n",
    "                    count = split_by_period_count(msg) if msg else 1\n",
    "                    groups[group] += count\n",
    "            elif isinstance(val, bool):\n",
    "                if val is False:\n",
    "                    groups[group] += 1\n",
    "\n",
    "    # Nested constraints\n",
    "    nested = fullOutput.get(\"constraints\", {})\n",
    "    if isinstance(nested, dict):\n",
    "        for key, val in nested.items():\n",
    "            group = CONSTRAINT_GROUPS.get(key)\n",
    "            if group:\n",
    "                if isinstance(val, dict) and \"passed\" in val:\n",
    "                    if not val[\"passed\"]:\n",
    "                        msg = val.get(\"message\", \"\")\n",
    "                        count = split_by_period_count(msg) if msg else 1\n",
    "                        groups[group] += count\n",
    "                elif isinstance(val, bool):\n",
    "                    if val is False:\n",
    "                        groups[group] += 1\n",
    "    return groups\n",
    "\n",
    "def count_grouped_passed(fullOutput: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Similar to count_grouped_errors, but we count how many constraints in each group are PASSED.\n",
    "    Return a dict: { \"Structural\": X, \"Logical\": Y, \"Attribute\": Z } indicating how many passed.\n",
    "    \"\"\"\n",
    "    passed_counts = {\"Structural\": 0, \"Logical\": 0, \"Attribute\": 0}\n",
    "\n",
    "    # Top-level\n",
    "    for key, val in fullOutput.items():\n",
    "        if key in [\"errors\", \"overall_passed\"]:\n",
    "            continue\n",
    "        group = CONSTRAINT_GROUPS.get(key)\n",
    "        if group:\n",
    "            if isinstance(val, dict) and \"passed\" in val:\n",
    "                if val[\"passed\"]:\n",
    "                    passed_counts[group] += 1\n",
    "            elif isinstance(val, bool):\n",
    "                if val is True:\n",
    "                    passed_counts[group] += 1\n",
    "\n",
    "    # Nested constraints\n",
    "    nested = fullOutput.get(\"constraints\", {})\n",
    "    if isinstance(nested, dict):\n",
    "        for key, val in nested.items():\n",
    "            group = CONSTRAINT_GROUPS.get(key)\n",
    "            if group:\n",
    "                if isinstance(val, dict) and \"passed\" in val:\n",
    "                    if val[\"passed\"]:\n",
    "                        passed_counts[group] += 1\n",
    "                elif isinstance(val, bool):\n",
    "                    if val is True:\n",
    "                        passed_counts[group] += 1\n",
    "\n",
    "    return passed_counts\n",
    "\n",
    "###############################################################################\n",
    "# 5. PER-RUN PARSING\n",
    "###############################################################################\n",
    "\n",
    "def parse_json_file(filepath: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    Parse a single JSON file => produce a run-level summary row with:\n",
    "      - overall_passed\n",
    "      - constraints_passed, constraints_total\n",
    "      - error_count\n",
    "      - structural_errors, logical_errors, attribute_errors\n",
    "      - structural_constraints_passed, logical_constraints_passed, attribute_constraints_passed\n",
    "    We ignore fullOutput[\"errors\"] array for counting errors.\n",
    "    \"\"\"\n",
    "    filename = os.path.basename(filepath)\n",
    "    m = FILENAME_REGEX.match(filename)\n",
    "    if not m:\n",
    "        return None\n",
    "    problem_raw = m.group(1)\n",
    "    model_name = m.group(2)\n",
    "    prompt_type = m.group(3)\n",
    "    run_number = int(m.group(4))\n",
    "    if not (1 <= run_number <= 5):\n",
    "        return None\n",
    "\n",
    "    problem_name = PROBLEM_NAME_MAP.get(problem_raw, problem_raw)\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    overall_passed = False\n",
    "    fo = data.get(\"fullOutput\", {})\n",
    "    if isinstance(data.get(\"result\"), bool):\n",
    "        overall_passed = data[\"result\"]\n",
    "    if isinstance(fo.get(\"overall_passed\"), bool):\n",
    "        overall_passed = fo[\"overall_passed\"]\n",
    "\n",
    "    # Count how many constraints are passed/total (for info)\n",
    "    constraints_passed = 0\n",
    "    constraints_total = 0\n",
    "    if isinstance(fo, dict):\n",
    "        # top-level\n",
    "        for key, val in fo.items():\n",
    "            if key in [\"errors\", \"overall_passed\"]:\n",
    "                continue\n",
    "            if isinstance(val, dict) and \"passed\" in val:\n",
    "                constraints_total += 1\n",
    "                if val[\"passed\"]:\n",
    "                    constraints_passed += 1\n",
    "            elif isinstance(val, bool):\n",
    "                constraints_total += 1\n",
    "                if val:\n",
    "                    constraints_passed += 1\n",
    "        # nested constraints\n",
    "        nested = fo.get(\"constraints\", {})\n",
    "        if isinstance(nested, dict):\n",
    "            for key, val in nested.items():\n",
    "                if isinstance(val, dict) and \"passed\" in val:\n",
    "                    constraints_total += 1\n",
    "                    if val[\"passed\"]:\n",
    "                        constraints_passed += 1\n",
    "                elif isinstance(val, bool):\n",
    "                    constraints_total += 1\n",
    "                    if val:\n",
    "                        constraints_passed += 1\n",
    "\n",
    "    # Overall error_count from constraints only\n",
    "    error_count = count_constraints_errors(fo)\n",
    "\n",
    "    # Grouped error counts (failing constraints)\n",
    "    group_errs = count_grouped_errors(fo)\n",
    "\n",
    "    # Grouped pass counts (passing constraints)\n",
    "    group_passed = count_grouped_passed(fo)\n",
    "\n",
    "    return {\n",
    "        \"filename\": filename,\n",
    "        \"problem\": problem_name,\n",
    "        \"model\": model_name,\n",
    "        \"prompt_type\": prompt_type,\n",
    "        \"run_number\": run_number,\n",
    "        \"overall_passed\": overall_passed,\n",
    "        \"constraints_passed\": constraints_passed,\n",
    "        \"constraints_total\": constraints_total,\n",
    "        \"error_count\": error_count,\n",
    "\n",
    "        # failing constraints\n",
    "        \"structural_errors\": group_errs[\"Structural\"],\n",
    "        \"logical_errors\": group_errs[\"Logical\"],\n",
    "        \"attribute_errors\": group_errs[\"Attribute\"],\n",
    "\n",
    "        # passing constraints\n",
    "        \"structural_constraints_passed\": group_passed[\"Structural\"],\n",
    "        \"logical_constraints_passed\": group_passed[\"Logical\"],\n",
    "        \"attribute_constraints_passed\": group_passed[\"Attribute\"]\n",
    "    }\n",
    "\n",
    "###############################################################################\n",
    "# 6. BUILD PER-RUN SUMMARY\n",
    "###############################################################################\n",
    "\n",
    "def build_summary_csv(input_glob: str, output_csv: str):\n",
    "    rows = []\n",
    "    for filepath in glob.glob(input_glob, recursive=True):\n",
    "        if filepath.endswith(\".json\"):\n",
    "            row = parse_json_file(filepath)\n",
    "            if row is not None:\n",
    "                rows.append(row)\n",
    "    # remove duplicates\n",
    "    seen = set()\n",
    "    unique_rows = []\n",
    "    for r in rows:\n",
    "        combo = (r[\"problem\"], r[\"model\"], r[\"prompt_type\"], r[\"run_number\"])\n",
    "        if combo not in seen:\n",
    "            seen.add(combo)\n",
    "            unique_rows.append(r)\n",
    "    unique_rows.sort(key=lambda r: (r[\"problem\"], r[\"model\"], r[\"prompt_type\"], r[\"run_number\"]))\n",
    "\n",
    "    fieldnames = [\n",
    "        \"filename\", \"problem\", \"model\", \"prompt_type\", \"run_number\",\n",
    "        \"overall_passed\", \"constraints_passed\", \"constraints_total\", \"error_count\",\n",
    "        \"structural_errors\", \"logical_errors\", \"attribute_errors\",\n",
    "        \"structural_constraints_passed\", \"logical_constraints_passed\", \"attribute_constraints_passed\"\n",
    "    ]\n",
    "    with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(unique_rows)\n",
    "    print(f\"Wrote {len(unique_rows)} per-run summary rows to {output_csv}\")\n",
    "\n",
    "###############################################################################\n",
    "# 7. AGGREGATE INTO TOP-LINE\n",
    "###############################################################################\n",
    "\n",
    "def aggregate_grouped_summary(input_csv: str, output_csv: str):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    # group by problem, model, prompt_type\n",
    "    group_cols = [\"problem\", \"model\", \"prompt_type\"]\n",
    "    # keep only combos that have exactly 5 runs\n",
    "    run_counts = df.groupby(group_cols)[\"run_number\"].nunique().reset_index().rename(columns={\"run_number\": \"run_count\"})\n",
    "    valid_groups = run_counts[run_counts[\"run_count\"] == 5][group_cols]\n",
    "    df_valid = pd.merge(df, valid_groups, on=group_cols, how=\"inner\")\n",
    "\n",
    "    agg = df_valid.groupby(group_cols, dropna=False).agg(\n",
    "        pass_rate = (\"overall_passed\", \"mean\"),\n",
    "        avg_error_count = (\"error_count\", \"mean\"),\n",
    "        avg_structural_errors = (\"structural_errors\", \"mean\"),\n",
    "        avg_logical_errors = (\"logical_errors\", \"mean\"),\n",
    "        avg_attribute_errors = (\"attribute_errors\", \"mean\"),\n",
    "\n",
    "        avg_structural_passed = (\"structural_constraints_passed\", \"mean\"),\n",
    "        avg_logical_passed = (\"logical_constraints_passed\", \"mean\"),\n",
    "        avg_attribute_passed = (\"attribute_constraints_passed\", \"mean\"),\n",
    "\n",
    "        run_count = (\"run_number\", \"count\")\n",
    "    ).reset_index()\n",
    "\n",
    "    # convert pass_rate to percentage and round\n",
    "    agg[\"pass_rate\"] = (agg[\"pass_rate\"] * 100).round(1)\n",
    "    agg[\"avg_error_count\"] = agg[\"avg_error_count\"].round(2)\n",
    "    agg[\"avg_structural_errors\"] = agg[\"avg_structural_errors\"].round(2)\n",
    "    agg[\"avg_logical_errors\"] = agg[\"avg_logical_errors\"].round(2)\n",
    "    agg[\"avg_attribute_errors\"] = agg[\"avg_attribute_errors\"].round(2)\n",
    "\n",
    "    agg[\"avg_structural_passed\"] = agg[\"avg_structural_passed\"].round(2)\n",
    "    agg[\"avg_logical_passed\"] = agg[\"avg_logical_passed\"].round(2)\n",
    "    agg[\"avg_attribute_passed\"] = agg[\"avg_attribute_passed\"].round(2)\n",
    "\n",
    "    agg.to_csv(output_csv, index=False)\n",
    "    print(f\"Wrote aggregated top-line summary to {output_csv}\")\n",
    "    print(agg.head(20))\n",
    "\n",
    "###############################################################################\n",
    "# 8. MAIN\n",
    "###############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_summary_csv(INPUT_GLOB, PER_RUN_CSV)\n",
    "    aggregate_grouped_summary(PER_RUN_CSV, AGGREGATED_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d77ff765",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummaries.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Example of what df might look like:\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#   model        prompt        constraint                 failed\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 0 GPT4omini    direct        vehicle_capacity           1\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 2. Group by (model, prompt, constraint) to compute average failures\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#    If you have multiple runs per combination, you can average them:\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m df_grouped \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconstraint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Now df_grouped has columns: model, prompt, constraint, failed (float 0..1)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 3. Create a pivoted table:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#    Rows = model × prompt (concatenate them into one label if you like)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#    Columns = constraint\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#    Values = average failure rate\u001b[39;00m\n\u001b[1;32m     26\u001b[0m df_grouped[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_grouped[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m df_grouped[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:8402\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[1;32m   8399\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   8400\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[0;32m-> 8402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   8403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8405\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8408\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8410\u001b[0m \u001b[43m    \u001b[49m\u001b[43msqueeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8413\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:965\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrouper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_grouper\n\u001b[0;32m--> 965\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmutated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmutated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m obj\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/groupby/grouper.py:888\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[1;32m    886\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m    891\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'prompt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load your CSV data\n",
    "# Replace \"summaries.csv\" with the filename that contains your constraint-level pass/fail data\n",
    "df = pd.read_csv(\"summaries.csv\")\n",
    "\n",
    "# Example of what df might look like:\n",
    "#   model        prompt        constraint                 failed\n",
    "# 0 GPT4omini    direct        vehicle_capacity           1\n",
    "# 1 GPT4omini    direct        connectivity               0\n",
    "# 2 GPT4omini    direct        cost_optimization          0\n",
    "# ... etc.\n",
    "\n",
    "# 2. Group by (model, prompt, constraint) to compute average failures\n",
    "#    If you have multiple runs per combination, you can average them:\n",
    "df_grouped = df.groupby([\"model\", \"prompt\", \"constraint\"], as_index=False)[\"failed\"].mean()\n",
    "\n",
    "# Now df_grouped has columns: model, prompt, constraint, failed (float 0..1)\n",
    "\n",
    "# 3. Create a pivoted table:\n",
    "#    Rows = model × prompt (concatenate them into one label if you like)\n",
    "#    Columns = constraint\n",
    "#    Values = average failure rate\n",
    "df_grouped[\"model_prompt\"] = df_grouped[\"model\"] + \"_\" + df_grouped[\"prompt\"]\n",
    "df_pivot = df_grouped.pivot(index=\"model_prompt\", columns=\"constraint\", values=\"failed\")\n",
    "\n",
    "# 4. Plot the heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(df_pivot, annot=True, cmap=\"Reds\", vmin=0, vmax=1,\n",
    "            cbar_kws={\"label\": \"Failure Rate (0=Perfect, 1=All Failed)\"})\n",
    "\n",
    "plt.title(\"Constraint Failure Heatmap\")\n",
    "plt.xlabel(\"Constraint\")\n",
    "plt.ylabel(\"Model + Prompt\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9547c50",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aggregated_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Pivot data for visualization with x-axis as prompt types instead of problems\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m structural_data_prompt \u001b[38;5;241m=\u001b[39m \u001b[43maggregated_df\u001b[49m\u001b[38;5;241m.\u001b[39mpivot_table(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_structural_passed\u001b[39m\u001b[38;5;124m\"\u001b[39m, aggfunc\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39msum)\n\u001b[1;32m      3\u001b[0m logical_data_prompt \u001b[38;5;241m=\u001b[39m aggregated_df\u001b[38;5;241m.\u001b[39mpivot_table(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_logical_passed\u001b[39m\u001b[38;5;124m\"\u001b[39m, aggfunc\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39msum)\n\u001b[1;32m      4\u001b[0m attribute_data_prompt \u001b[38;5;241m=\u001b[39m aggregated_df\u001b[38;5;241m.\u001b[39mpivot_table(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_attribute_passed\u001b[39m\u001b[38;5;124m\"\u001b[39m, aggfunc\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39msum)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aggregated_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Pivot data for visualization with x-axis as prompt types instead of problems\n",
    "structural_data_prompt = aggregated_df.pivot_table(index=\"model\", columns=\"prompt_type\", values=\"avg_structural_passed\", aggfunc=np.sum)\n",
    "logical_data_prompt = aggregated_df.pivot_table(index=\"model\", columns=\"prompt_type\", values=\"avg_logical_passed\", aggfunc=np.sum)\n",
    "attribute_data_prompt = aggregated_df.pivot_table(index=\"model\", columns=\"prompt_type\", values=\"avg_attribute_passed\", aggfunc=np.sum)\n",
    "\n",
    "# Creating three subfigures (one for each constraint type) with prompt types on x-axis\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Structural Constraints\n",
    "sns.heatmap(structural_data_prompt, annot=True, cmap=\"Blues\", linewidths=0.5, fmt=\".1f\", ax=axes[0])\n",
    "axes[0].set_title(\"Structural Constraints Passed\")\n",
    "axes[0].set_xlabel(\"Prompt Type\")\n",
    "axes[0].set_ylabel(\"Model\")\n",
    "\n",
    "# Logical Constraints\n",
    "sns.heatmap(logical_data_prompt, annot=True, cmap=\"Greens\", linewidths=0.5, fmt=\".1f\", ax=axes[1])\n",
    "axes[1].set_title(\"Logical Constraints Passed\")\n",
    "axes[1].set_xlabel(\"Prompt Type\")\n",
    "axes[1].set_ylabel(\"Model\")\n",
    "\n",
    "# Attribute Constraints\n",
    "sns.heatmap(attribute_data_prompt, annot=True, cmap=\"Reds\", linewidths=0.5, fmt=\".1f\", ax=axes[2])\n",
    "axes[2].set_title(\"Attribute Constraints Passed\")\n",
    "axes[2].set_xlabel(\"Prompt Type\")\n",
    "axes[2].set_ylabel(\"Model\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063b7423",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
